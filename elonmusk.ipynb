{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pycontractions  import Contractions\n",
    "import string\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Tweets Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2668 entries, 0 to 2667\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Tweets          2668 non-null   object        \n",
      " 1   Retweets        2668 non-null   int64         \n",
      " 2   Likes           2668 non-null   int64         \n",
      " 3   Date            2668 non-null   datetime64[ns]\n",
      " 4   Cleaned_Tweets  2668 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 104.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/cleandata.csv', parse_dates=['Date'],encoding = \"utf-8\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_info(dataframe):\n",
    "    cols_list = dataframe.columns.tolist()\n",
    "    data_types = df.dtypes    \n",
    "    \n",
    "    for col in cols_list:        \n",
    "        # get the number of unique entries for each column\n",
    "        no_unique_value = dataframe[col].nunique()\n",
    "        print(f'The number of unique values for column {col} is {no_unique_value}')\n",
    "\n",
    "        # Check for the data type and get the min and max value\n",
    "        if data_types[col] != 'object':\n",
    "            min_value = dataframe[col].min()\n",
    "            max_value = dataframe[col].max()\n",
    "            print(f'The min value for column {col} is {min_value}')\n",
    "            print(f'The max value for column {col} is {max_value}\\n')\n",
    "        else:\n",
    "            print('\\n')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique values for column Tweets is 2642\n",
      "\n",
      "\n",
      "The number of unique values for column Retweets is 1834\n",
      "The min value for column Retweets is 41\n",
      "The max value for column Retweets is 681707\n",
      "\n",
      "The number of unique values for column Likes is 2598\n",
      "The min value for column Likes is 933\n",
      "The max value for column Likes is 4780787\n",
      "\n",
      "The number of unique values for column Date is 2668\n",
      "The min value for column Date is 2022-01-27 21:00:09\n",
      "The max value for column Date is 2022-10-27 16:17:39\n",
      "\n",
      "The number of unique values for column Cleaned_Tweets is 2382\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_basic_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = Contractions('GoogleNews-vectors-negative300.bin')\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df['Cleaned_Tweets'] = df['Cleaned_Tweets'].replace({r'[^\\x00-\\x7F]+':''}, regex=True)\n",
    "\n",
    "def clean(text):\n",
    "    text = text.strip()\n",
    "    # Fix quotes\n",
    "    text = text.replace(\"’\", \"'\") \\\n",
    "        .replace(\"‘\", \"'\") \\\n",
    "        .replace(\"”\", '\"') \\\n",
    "        .replace(\"“\", '\"')\n",
    "\n",
    "    # Replace &amp; with and\n",
    "    text = text.replace('&amp;','and')\n",
    "\n",
    "    text = text.replace('&gt;', 'greater than ')\n",
    "\n",
    "    # Fix sentences which does not have space after full stop\n",
    "    text = text.replace('.', '. ')\n",
    "\n",
    "    text = text.replace('-', '')\n",
    "    \n",
    "    # Fix contractions\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "\n",
    "    # Remove punctuations\n",
    "    # text = \"\".join([i for i in text if i not in string.punctuation]) \n",
    "    no_spaces = len(string.punctuation)\n",
    "       \n",
    "    text = text.translate(str.maketrans(string.punctuation,' ' * no_spaces))  \n",
    "    return text\n",
    "\n",
    "df['Cleaned_Tweets'] = df['Cleaned_Tweets'].apply(clean)\n",
    "# Remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop += ['im','ie','ete', 'dont', 'cant', 'would','wont','doesnt','must','might','also','almost','so', 'haha']\n",
    "df['Cleaned_Tweets'] = df['Cleaned_Tweets'].str.lower().apply(lambda x : \" \".join([word for word in x.split() if word not in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   thanks\n",
       "1                               absolutely\n",
       "2                 dear twitter advertisers\n",
       "3    meeting lot cool people twitter today\n",
       "4             entering twitter hq let sink\n",
       "Name: Cleaned_Tweets, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cleaned_Tweets'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/after_clean.csv',encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Token_Counts'] = df['Cleaned_Tweets'].apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets with less than 3 tokens\n",
    "df = df[df['Token_Counts'] > 3].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1551, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model pipeline\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Emanuel/bertweet-emotion-base\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Emanuel/bertweet-emotion-base\"\n",
    ")\n",
    "device = -1 #torch.cuda.current_device() if torch.cuda.is_available else -1\n",
    "model_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [{'label': 'joy', 'score': 0.9898011088371277}...\n",
       "1    [{'label': 'joy', 'score': 0.7461026310920715}...\n",
       "2    [{'label': 'anger', 'score': 0.441839307546615...\n",
       "3    [{'label': 'joy', 'score': 0.7306174635887146}...\n",
       "4    [{'label': 'anger', 'score': 0.632644712924957...\n",
       "Name: Emotion_Scores, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion_Scores'] = model_pipeline(df['Cleaned_Tweets'].to_list(), top_k=None)\n",
    "df['Emotion_Scores'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      joy\n",
       "1      joy\n",
       "2    anger\n",
       "3      joy\n",
       "4    anger\n",
       "Name: Emotion1, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign top 2 emotions\n",
    "df['Emotion1'] = df['Emotion_Scores'].apply(lambda x: x[0]['label'])\n",
    "df['Emotion1'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()\n",
    "vectorizer = KeyphraseCountVectorizer(pos_pattern='<NNP.*>+')\n",
    "df['Noun_Keyphrases_Score'] = kw_model.extract_keywords(docs=df['Cleaned_Tweets'].to_list(), vectorizer=vectorizer, stop_words='english', top_n=5)\n",
    "df['Noun_Keyphrases'] = df['Noun_Keyphrases_Score'].apply(lambda record: [x[0] for x in record])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twitter', 0.5599), ('cool', 0.2777)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Noun_Keyphrases_Score'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [twitter, cool]\n",
       "1               [twitter]\n",
       "2               [twitter]\n",
       "3               [twitter]\n",
       "4                   [fan]\n",
       "              ...        \n",
       "1546                   []\n",
       "1547          [manganese]\n",
       "1548    [manganese, iron]\n",
       "1549                   []\n",
       "1550          [cb radios]\n",
       "Name: Noun_Keyphrases, Length: 1551, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Noun_Keyphrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting score types for serialization\n",
    "def fix_float_type(input):\n",
    "    return [(x[0], str(x[1])) for x in input]\n",
    "df['Noun_Keyphrases_Score'] = df['Noun_Keyphrases_Score'].apply(fix_float_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'twitter': 84,\n",
       "         'cool': 10,\n",
       "         'fan': 7,\n",
       "         'new york times': 1,\n",
       "         'nonfake vitalik tweet': 1,\n",
       "         'doubletake': 1,\n",
       "         'fair': 3,\n",
       "         'silicon valley': 1,\n",
       "         'fog war': 2,\n",
       "         'war': 19,\n",
       "         'douche': 1,\n",
       "         'house': 5,\n",
       "         'kasparov': 1,\n",
       "         'good': 57,\n",
       "         'dod': 4,\n",
       "         'algorithm': 9,\n",
       "         'neuralink': 5,\n",
       "         'nov': 1,\n",
       "         'switzerland belgium': 1,\n",
       "         'beta': 26,\n",
       "         'release': 21,\n",
       "         'tbh': 7,\n",
       "         'gps': 4,\n",
       "         'headline': 1,\n",
       "         'world': 30,\n",
       "         'un ass': 1,\n",
       "         'un': 5,\n",
       "         'giga berlin': 4,\n",
       "         'many': 63,\n",
       "         'tesla': 138,\n",
       "         'putin': 2,\n",
       "         'kremlin nice': 1,\n",
       "         'russia overrunning ukraine': 1,\n",
       "         'ukraine': 26,\n",
       "         'russia': 22,\n",
       "         'crimea': 10,\n",
       "         'cuba': 1,\n",
       "         'falcon': 16,\n",
       "         'vox populi vox dei': 1,\n",
       "         'hope': 7,\n",
       "         'wrong analogy america humiliating withdrawal afghanistan one america': 1,\n",
       "         'america crimea': 1,\n",
       "         'russia russia': 2,\n",
       "         'ww3': 3,\n",
       "         'crimea scenario': 1,\n",
       "         'nato': 1,\n",
       "         'russia russia crimea': 1,\n",
       "         'russia southern navy base': 1,\n",
       "         'us': 25,\n",
       "         'rv': 1,\n",
       "         'satellite': 5,\n",
       "         'rud': 3,\n",
       "         'lonely': 1,\n",
       "         'venn diagram': 1,\n",
       "         'das boot boss': 1,\n",
       "         'das': 2,\n",
       "         'seas': 5,\n",
       "         'deescalate': 2,\n",
       "         'ukraine govt': 1,\n",
       "         'super': 22,\n",
       "         'usa europe': 1,\n",
       "         'europe': 4,\n",
       "         'usa': 3,\n",
       "         'communications': 5,\n",
       "         'defend': 2,\n",
       "         'leo': 2,\n",
       "         'boeing': 1,\n",
       "         'viasat': 2,\n",
       "         'chill laugh share': 1,\n",
       "         'terminal': 6,\n",
       "         'long': 24,\n",
       "         'sam': 1,\n",
       "         'pointless': 1,\n",
       "         'shield': 2,\n",
       "         'autopilot office palo alto': 1,\n",
       "         'palo alto': 2,\n",
       "         'bf16': 1,\n",
       "         'clear': 10,\n",
       "         'ft': 1,\n",
       "         'tesla ai': 9,\n",
       "         'ai': 33,\n",
       "         'dec': 3,\n",
       "         'helium': 1,\n",
       "         'supply crimea': 2,\n",
       "         'russia inaccurate': 1,\n",
       "         'prorussia party': 2,\n",
       "         'prorussia': 2,\n",
       "         'party': 18,\n",
       "         'gm ford': 2,\n",
       "         'gm': 4,\n",
       "         'bs': 3,\n",
       "         'news remote': 1,\n",
       "         'kosovo': 1,\n",
       "         'un pick': 1,\n",
       "         'victory ukraine': 1,\n",
       "         'donbas crimea': 1,\n",
       "         'bot': 9,\n",
       "         'bot attack poll': 1,\n",
       "         'russia ukraine russia': 1,\n",
       "         'ukrainerussia': 1,\n",
       "         'un supervision russia': 1,\n",
       "         'wellsaid': 1,\n",
       "         'road inject': 1,\n",
       "         'scifi': 3,\n",
       "         'prufrock ii': 1,\n",
       "         'russians america': 1,\n",
       "         'america': 8,\n",
       "         'funny': 7,\n",
       "         'trois': 1,\n",
       "         'optimus': 3,\n",
       "         'hardware': 9,\n",
       "         'california': 8,\n",
       "         'starbase south padre': 1,\n",
       "         'cybertruck waterproof': 1,\n",
       "         'boat cross': 1,\n",
       "         'motors': 2,\n",
       "         'cool hardware demos': 1,\n",
       "         'kph': 1,\n",
       "         'tesla megapack': 1,\n",
       "         'hawaii': 1,\n",
       "         'karma': 1,\n",
       "         'henrik': 1,\n",
       "         'joke slow': 1,\n",
       "         'son saxon': 2,\n",
       "         'lenin': 2,\n",
       "         'species': 4,\n",
       "         'weems st': 1,\n",
       "         'st': 3,\n",
       "         'austin': 6,\n",
       "         'longterm': 5,\n",
       "         'area south giga river': 1,\n",
       "         'giga texas river': 1,\n",
       "         'giga texas': 4,\n",
       "         'paradise': 1,\n",
       "         'texas': 7,\n",
       "         'south africa': 1,\n",
       "         'africa': 2,\n",
       "         'q2': 2,\n",
       "         'brownsville': 2,\n",
       "         'november': 4,\n",
       "         'bro': 4,\n",
       "         'fsd': 22,\n",
       "         'us canada': 2,\n",
       "         'canada': 8,\n",
       "         'summon autopark': 1,\n",
       "         '160k': 1,\n",
       "         'prudencia': 1,\n",
       "         'bay': 7,\n",
       "         'fed': 3,\n",
       "         'paletteblue': 1,\n",
       "         'antarctica': 2,\n",
       "         'gen starlink constellation': 1,\n",
       "         'kg': 1,\n",
       "         'alexander': 1,\n",
       "         'hyperloop': 4,\n",
       "         'zatko': 1,\n",
       "         'chat': 1,\n",
       "         'infinite': 3,\n",
       "         'joe mode': 1,\n",
       "         'realworld': 4,\n",
       "         'fleet australia': 1,\n",
       "         'tesla north america': 1,\n",
       "         'north america': 2,\n",
       "         'sun': 7,\n",
       "         'km': 2,\n",
       "         'ground mount rooftop': 1,\n",
       "         'love graffiti art giga berlin': 1,\n",
       "         'powerwall': 4,\n",
       "         'sun shine wind blow': 1,\n",
       "         'hydro': 1,\n",
       "         'thermonuclear': 1,\n",
       "         'spam': 13,\n",
       "         'david sacks': 1,\n",
       "         'lord rings hobbit': 1,\n",
       "         'peter jackson excellent': 1,\n",
       "         'jackson': 2,\n",
       "         'south korea': 3,\n",
       "         'population': 10,\n",
       "         'chris': 1,\n",
       "         'h2': 1,\n",
       "         'imo deltav': 1,\n",
       "         'accurate': 8,\n",
       "         'imo': 13,\n",
       "         'jitter hardware command': 1,\n",
       "         '1k': 1,\n",
       "         'arc': 1,\n",
       "         'hamlet hamlet': 1,\n",
       "         'hamlet': 1,\n",
       "         'national security standpoint': 1,\n",
       "         'universe': 6,\n",
       "         'mass': 11,\n",
       "         'monday': 1,\n",
       "         'james': 2,\n",
       "         'mechazilla': 1,\n",
       "         'board': 10,\n",
       "         'tell october': 1,\n",
       "         'austin san antonio': 1,\n",
       "         'vegas': 4,\n",
       "         'sept': 2,\n",
       "         'north america september': 1,\n",
       "         'tesla autopilot ai': 2,\n",
       "         'tesla autopilot': 3,\n",
       "         'po': 1,\n",
       "         'bill nix prof stanford': 1,\n",
       "         'grad': 1,\n",
       "         'thermal protection': 2,\n",
       "         'manchester united ur': 1,\n",
       "         'ur': 3,\n",
       "         'democratic party': 2,\n",
       "         'republican party': 1,\n",
       "         'republican': 7,\n",
       "         '400k': 1,\n",
       "         'congrats giga shanghai': 1,\n",
       "         'b7': 1,\n",
       "         'tesla advertise': 2,\n",
       "         'swallows': 1,\n",
       "         'tesla fremont': 3,\n",
       "         'fremont': 3,\n",
       "         'fake': 13,\n",
       "         'lt': 8,\n",
       "         'sec': 3,\n",
       "         'smell': 2,\n",
       "         'austin international airport silly': 1,\n",
       "         'ratio digital': 1,\n",
       "         'cnbc': 1,\n",
       "         'jon stewart daily show colbert': 1,\n",
       "         'quinn': 1,\n",
       "         'sub lightspeed': 1,\n",
       "         'god': 1,\n",
       "         'wsj': 3,\n",
       "         'sergey': 3,\n",
       "         'satan source psychic nicole': 1,\n",
       "         'sickonolfis pack attack chihuahuas': 1,\n",
       "         'supernova': 1,\n",
       "         'karaoke': 1,\n",
       "         'das bootdas': 1,\n",
       "         'fbi': 3,\n",
       "         'fud': 2,\n",
       "         'bs sergey': 1,\n",
       "         'disney': 2,\n",
       "         'ferdinand pich': 1,\n",
       "         'complex': 7,\n",
       "         'china': 9,\n",
       "         'airbnb': 1,\n",
       "         'building giga nevada rocks': 1,\n",
       "         'tesla fremont giga nevada': 1,\n",
       "         'starlink gamechanger': 1,\n",
       "         'charlie ergen': 1,\n",
       "         'dome': 1,\n",
       "         'kind': 6,\n",
       "         'tvc': 2,\n",
       "         'hydraulics mass': 1,\n",
       "         'eg': 3,\n",
       "         'safety know': 1,\n",
       "         'american caesar masters': 1,\n",
       "         'red': 4,\n",
       "         'biden': 2,\n",
       "         'united states america desantis': 1,\n",
       "         'desantis': 2,\n",
       "         'united states': 2,\n",
       "         'bull china shop situation': 1,\n",
       "         'sunset dems': 1,\n",
       "         'hang hat': 1,\n",
       "         'hate': 8,\n",
       "         'trumps': 1,\n",
       "         'hat': 3,\n",
       "         'mimic dark': 1,\n",
       "         'acid': 2,\n",
       "         'douglas adams': 1,\n",
       "         'tokyo': 1,\n",
       "         'beijing xian': 1,\n",
       "         'china japan': 1,\n",
       "         'japan': 3,\n",
       "         '150k': 1,\n",
       "         'roof': 3,\n",
       "         'van': 2,\n",
       "         'doc': 1,\n",
       "         'mike duncan': 1,\n",
       "         'orwell': 1,\n",
       "         'doge': 4,\n",
       "         'sock': 4,\n",
       "         'sock con': 1,\n",
       "         'sock tech': 1,\n",
       "         'pronuclear': 1,\n",
       "         'elvis elvish': 1,\n",
       "         'talulah': 1,\n",
       "         'venice': 2,\n",
       "         'attempt bait': 1,\n",
       "         'cellular': 1,\n",
       "         'beverly hills': 1,\n",
       "         'wifi': 2,\n",
       "         'comme': 1,\n",
       "         'merch': 4,\n",
       "         'congratulations giga berlin team': 1,\n",
       "         'son sjm': 1,\n",
       "         'monocle': 1,\n",
       "         'flash pan potential': 1,\n",
       "         'yang': 1,\n",
       "         'pac': 1,\n",
       "         'mayra': 1,\n",
       "         'sn24 high bay': 1,\n",
       "         'tesla fsd': 3,\n",
       "         'august': 1,\n",
       "         'high bay mega bay': 1,\n",
       "         'boca chica area': 1,\n",
       "         'merlin': 1,\n",
       "         'world party': 1,\n",
       "         'bay area': 3,\n",
       "         'multiplayer': 1,\n",
       "         'mdma': 1,\n",
       "         'diet coke': 1,\n",
       "         'usb': 1,\n",
       "         'recirc range': 1,\n",
       "         'wolf': 1,\n",
       "         'mistake': 2,\n",
       "         'solar powerwall': 1,\n",
       "         'life 8k sjm': 1,\n",
       "         'china onechild policy china': 1,\n",
       "         'wall': 4,\n",
       "         'skull': 1,\n",
       "         'dispenses pez merch': 1,\n",
       "         'steward species': 1,\n",
       "         'wheel movement vision': 1,\n",
       "         'eu': 1,\n",
       "         'lefts': 2,\n",
       "         'north american': 1,\n",
       "         'nv': 1,\n",
       "         'el camino jack box': 1,\n",
       "         'jack': 4,\n",
       "         'getty watermark coup': 1,\n",
       "         'press': 2,\n",
       "         'johnny rotten': 1,\n",
       "         'min ages house senate president': 1,\n",
       "         'max': 2,\n",
       "         'bob': 2,\n",
       "         'republican democrat confident': 1,\n",
       "         'democrat': 3,\n",
       "         'americans': 1,\n",
       "         'rick caruso': 1,\n",
       "         'los angeles': 1,\n",
       "         'ivory tower stuff': 1,\n",
       "         'nns': 2,\n",
       "         'illustrate': 1,\n",
       "         'ceo': 4,\n",
       "         'harris': 1,\n",
       "         'paypal march': 1,\n",
       "         'paypal': 2,\n",
       "         'igpu': 1,\n",
       "         'wikipedia': 2,\n",
       "         'esg': 7,\n",
       "         'bro billys': 1,\n",
       "         'ur lame': 1,\n",
       "         'ok buddy': 1,\n",
       "         'mackenzie ahem': 1,\n",
       "         'looney': 2,\n",
       "         'mouth': 1,\n",
       "         'truth': 4,\n",
       "         'toilet implicit': 1,\n",
       "         'maximize': 3,\n",
       "         'democrats': 1,\n",
       "         'hillary': 1,\n",
       "         'james accurate tesla': 1,\n",
       "         'filesystem': 1,\n",
       "         'linux': 1,\n",
       "         'yaw': 1,\n",
       "         'nigeria': 1,\n",
       "         'tesla audio': 1,\n",
       "         'union': 2,\n",
       "         'good cox': 1,\n",
       "         'int dex talismans': 1,\n",
       "         'dna': 2,\n",
       "         'iron': 4,\n",
       "         'comparison postcompression dna': 1,\n",
       "         'italy': 1,\n",
       "         'south korea hong kong': 1,\n",
       "         'usa birth': 1,\n",
       "         'netflix documentary': 1,\n",
       "         'orbit space': 1,\n",
       "         'katana post pic': 1,\n",
       "         'twitter account cheesy secret instagram account': 1,\n",
       "         'california tesla': 2,\n",
       "         'worried': 2,\n",
       "         'russians ok': 1,\n",
       "         'mi6': 1,\n",
       "         'clinton': 5,\n",
       "         'hardcore litigation department': 1,\n",
       "         'russia hoax': 2,\n",
       "         'hoax': 4,\n",
       "         'sussmann': 2,\n",
       "         'russia hoax fbi': 1,\n",
       "         'tweet clinton': 1,\n",
       "         'brazil': 1,\n",
       "         'obv diehard dem': 1,\n",
       "         'bi': 1,\n",
       "         'maestro': 1,\n",
       "         'republicans': 2,\n",
       "         'control house senate president': 1,\n",
       "         'people america': 1,\n",
       "         'yale getsin wacktivist olympics': 1,\n",
       "         'kindness party': 1,\n",
       "         'republican watch': 1,\n",
       "         'yale epicenter': 1,\n",
       "         'sooo': 1,\n",
       "         'geese': 1,\n",
       "         'button': 2,\n",
       "         'broaden': 1,\n",
       "         'lib': 1,\n",
       "         'libs': 1,\n",
       "         'semirandom': 1,\n",
       "         'tesla china': 2,\n",
       "         'taj': 1,\n",
       "         'buzzkill issue': 1,\n",
       "         'knowin': 1,\n",
       "         'nazi': 1,\n",
       "         'space control code': 1,\n",
       "         'rumor': 1,\n",
       "         'parody': 1,\n",
       "         'twitter acquisition completes company': 1,\n",
       "         'infosec': 1,\n",
       "         'truth social': 3,\n",
       "         'passive': 4,\n",
       "         'george': 2,\n",
       "         'tesla 110k': 1,\n",
       "         'npc': 1,\n",
       "         'algorithms': 1,\n",
       "         'zip2': 1,\n",
       "         'nbc': 1,\n",
       "         'lenin train': 1,\n",
       "         'jack bogle vanguard fame': 1,\n",
       "         'ssris': 1,\n",
       "         'alcohol': 1,\n",
       "         'adderall': 2,\n",
       "         'obama president': 1,\n",
       "         'twitter dms end': 1,\n",
       "         'vegas boring co': 1,\n",
       "         'co': 3,\n",
       "         'washington post': 1,\n",
       "         'washington': 2,\n",
       "         'instagram curse': 1,\n",
       "         'san francisco office sec shameless': 1,\n",
       "         'wall st shortseller': 1,\n",
       "         'saudi': 2,\n",
       "         'cfo': 1,\n",
       "         'underground tunnels immune surface': 1,\n",
       "         'gen neuralinks': 1,\n",
       "         'tinnitus': 1,\n",
       "         'lt lt': 1,\n",
       "         'rybelsus': 1,\n",
       "         'ai nn': 1,\n",
       "         'electronics': 1,\n",
       "         'neuralinks motor sensory': 1,\n",
       "         'motor': 2,\n",
       "         'shadow ban council': 1,\n",
       "         'nyt': 1,\n",
       "         'crypto': 2,\n",
       "         'diess': 1,\n",
       "         'mgmt': 1,\n",
       "         'commercialize tzero': 1,\n",
       "         'wish jb': 1,\n",
       "         'jb': 2,\n",
       "         'walter isaacson': 1,\n",
       "         'eberhard': 3,\n",
       "         'mid': 1,\n",
       "         'july': 1,\n",
       "         'shell corp': 2,\n",
       "         'tesla motors': 1,\n",
       "         'ac propulsions tzero': 1,\n",
       "         'biz plan': 1,\n",
       "         'shadowy': 1,\n",
       "         'wall connector': 1,\n",
       "         'twitter board': 4,\n",
       "         'adapters mobile': 1,\n",
       "         'immediate takeaway': 1,\n",
       "         'refund': 1,\n",
       "         'subtract crypto scam': 1,\n",
       "         'tesla texas': 1,\n",
       "         'first giga texas': 1,\n",
       "         'gigafactory texas grand opening': 1,\n",
       "         'hz': 1,\n",
       "         'spain': 1,\n",
       "         '100mw': 1,\n",
       "         'george lucas': 1,\n",
       "         'wall berghain': 1,\n",
       "         'europe lhd summer rhd': 1,\n",
       "         'uaw': 4,\n",
       "         'halo smash bros thumb': 1,\n",
       "         'halo infinite': 2,\n",
       "         'terminator resistance': 2,\n",
       "         'halo infinite legendary': 1,\n",
       "         'deg longrange fov sigh cyberpunk': 1,\n",
       "         'good altho': 1,\n",
       "         'covid19 virus': 1,\n",
       "         'paywall': 1,\n",
       "         'wine barry white witty': 1,\n",
       "         'toronto': 1,\n",
       "         'twitter algorithm': 2,\n",
       "         '500k lb': 1,\n",
       "         'giga berlinbrandenburg': 2,\n",
       "         'limbic instinct': 1,\n",
       "         'gm chrysler': 1,\n",
       "         'ford tesla american': 1,\n",
       "         'imo hardcore realworld ai': 1,\n",
       "         'acceleration asics': 1,\n",
       "         'telegram post ramzan kadyrov': 1,\n",
       "         'chechen': 1,\n",
       "         'united earth': 1,\n",
       "         'copv': 1,\n",
       "         'psi': 1,\n",
       "         'plugs legacy': 1,\n",
       "         'pshaw pedestrian': 1,\n",
       "         'nikola tesla': 1,\n",
       "         'faraday': 1,\n",
       "         'block russian news': 1,\n",
       "         'ich mchte mich recht herzlich bedanken': 1,\n",
       "         'ist sehr': 1,\n",
       "         'ukraine civilian': 1,\n",
       "         'dec 24th': 1,\n",
       "         'saudi arabia': 1,\n",
       "         'fart': 1,\n",
       "         'neural nets cameras realworld': 1,\n",
       "         'compiler': 2,\n",
       "         'arenas': 2,\n",
       "         'south padre brownsville': 1,\n",
       "         'giga texas area': 1,\n",
       "         'irs': 1,\n",
       "         'faa': 1,\n",
       "         'survival': 1,\n",
       "         'asymmetry': 1,\n",
       "         'munger': 1,\n",
       "         'australia new zealand': 1,\n",
       "         'dave lee': 1,\n",
       "         'dave': 2,\n",
       "         'rogan fridman dodd': 1,\n",
       "         'hotel vegas convention': 1,\n",
       "         'gavin kind': 1,\n",
       "         'thursday': 1,\n",
       "         'holy grail rocketry': 1,\n",
       "         'progress starbase texas': 1,\n",
       "         'scan': 1,\n",
       "         'safety paramount': 1,\n",
       "         'harbinger battle': 1,\n",
       "         'denmark': 1,\n",
       "         'cartoonist': 1,\n",
       "         'manganese': 2,\n",
       "         'cb radios': 1})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "keyphrases = []\n",
    "for entry in df['Noun_Keyphrases'].values:\n",
    "    keyphrases += entry\n",
    "Counter(keyphrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:11:40,504 loading file C:\\Users\\baira\\.flair\\models\\pos-english\\a9a73f6cd878edce8a0fa518db76f441f1cc49c2525b2b4557af278ec2f0659e.121306ea62993d04cd1978398b68396931a39eb47754c8a06a87f325ea70ac63\n",
      "2022-12-04 19:11:41,334 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
     ]
    }
   ],
   "source": [
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/pos-english\")\n",
    "\n",
    "def flair_pos_tagging(sentence):\n",
    "    # print(sentence)\n",
    "    verbs = set()\n",
    "    adjectives = set()\n",
    "    sen = Sentence(sentence)\n",
    "    tagger.predict(sen)\n",
    "\n",
    "    for label in sen.get_labels('pos'):\n",
    "        \n",
    "        if label.value[0:2] == 'VB' and label.score > 0.75:\n",
    "            verbs.add(label.data_point.text)\n",
    "            # print(verbs)\n",
    "        if label.value[0:2] == 'JJ' and label.score > 0.75:\n",
    "            adjectives.add(label.data_point.text)\n",
    "            # print(adjectives)\n",
    "\n",
    "    return list(verbs), list(adjectives)\n",
    "\n",
    "df['verbs'], df['adjectives'] = zip(*df['Cleaned_Tweets'].str.lower().apply(flair_pos_tagging))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verbs</th>\n",
       "      <th>adjectives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[meeting]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[sink, entering]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[get, underappreciated]</td>\n",
       "      <td>[closer, local]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[empowers]</td>\n",
       "      <td>[able, beautiful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>[big]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>[gone, stripped]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>[requires, operates]</td>\n",
       "      <td>[higher, less]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>[]</td>\n",
       "      <td>[several, alternative, manganese]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>[paid]</td>\n",
       "      <td>[high, responsible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>[]</td>\n",
       "      <td>[free]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1551 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        verbs                         adjectives\n",
       "0                   [meeting]                                 []\n",
       "1            [sink, entering]                                 []\n",
       "2     [get, underappreciated]                    [closer, local]\n",
       "3                  [empowers]                  [able, beautiful]\n",
       "4                          []                              [big]\n",
       "...                       ...                                ...\n",
       "1546         [gone, stripped]                                 []\n",
       "1547     [requires, operates]                     [higher, less]\n",
       "1548                       []  [several, alternative, manganese]\n",
       "1549                   [paid]                [high, responsible]\n",
       "1550                       []                             [free]\n",
       "\n",
       "[1551 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['verbs','adjectives']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_to_remove = ['get','are','is','am','have','has','been','seen','had','do','took','be',\n",
    "                    'make','does','like','did','see','was','go','got','get','want','getting','gets', 'exist',\n",
    "                    'done','doing','went','uses','says','known','let','given' ,'gave','makes','goes',\n",
    "                    'gone','going','saw','being','were']\n",
    "\n",
    "def remove_words(row):\n",
    "    verbs_list = []\n",
    "\n",
    "    if len(row) > 0:\n",
    "        for i in row:\n",
    "            if i not in verbs_to_remove:\n",
    "                verbs_list.append(i)\n",
    "    return verbs_list                  \n",
    "\n",
    "df['verbs'] = df['verbs'].apply(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_to_remove = ['many','most','much','such']\n",
    "\n",
    "def remove_words(row):\n",
    "    adj_list = []\n",
    "\n",
    "    if len(row) > 0:\n",
    "        for i in row:\n",
    "            if i not in adj_to_remove:\n",
    "                adj_list.append(i)\n",
    "    return adj_list                  \n",
    "\n",
    "df['adjectives'] = df['adjectives'].apply(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_to_remove = ['ft', 'tbc', 'gt50' , '50year' , 'bro' , 'lt5', 'україна', 'thursday', '110k', 'friday' , 'tbh', 'qa', 'haha' ,\n",
    "                 'ac', 'november', 'sept' , 'lhd', 'rhd', 'вы', 'bf' , 'fsd' , 'us', 'fud', 'gwhdaykm2', 'irs', 'lib', 'libs', 'mgmt',\n",
    "                 'умом без сердца', 'fps', 'venn', 'cnc', 'hz' , 'def', '8k' , 'nighti', 'encanta el mariachi', 'forward10', 'ich',\n",
    "                 'isp', 'comme ci comme', 'la', 'ludendorffs', 'july', 'ðoge', 'siegel', 'zaporizhzhia', 'h2', 'ron barron', 'sjm', 'doj',\n",
    "                 'talulah', 'starlink eg league', 'areof pastpresentand future', 'mr president', 'bs', 'el camino jack', 'jb', 'nyt',\n",
    "                 'честью', 'das bootdas baby', 'zukunft', 'gnus', 'yang', 'august', 'reps', 'btw', 'wsj', 'covid19 anymorei',\n",
    "                 'ozempicrybelsus', 'дура', 'charlie ergen', 'dec', 'october', '1k', '100mw', 'mdma', 'un ass', 'donbas', 'gwynne',\n",
    "                 'ppmgt1000', 'же несчастная дура', 'awe', 'mf', 'kbg', 'nn', 'ch4', 'kg', 'bf16', 'thingreal', 'uaw', 'pshaw',\n",
    "                 'на этот', 'faa', 'row iv', 'ev', 'gm', 'kph', 'haha', 'b7', 'may', 'zatko', 'eu', 'un', 'nv', 'tw', 'gt95', 'mps',\n",
    "                 'q2', 'ur', 'путин', 'agoford', 'leavei', 'vw', 'ferdinand piëch', 'giga berlinbrandenburg', 'satan', 'das baby',\n",
    "                 'cape canaveralhumans', 'chad', 'hijinks']\n",
    "\n",
    "def remove_words(row):\n",
    "    noun_list = []\n",
    "\n",
    "    if len(row) > 0:\n",
    "        for i in row:\n",
    "            if i not in noun_to_remove:\n",
    "                noun_list.append(i)\n",
    "    return noun_list                  \n",
    "\n",
    "df['Noun_Keyphrases'] = df['Noun_Keyphrases'].apply(remove_words)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('./data/processed_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you very much. The future is very exciting'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "to_translate = 'Ich mÃchte mich recht herzlich bedanken  Die Zukunft ist sehr spannend'\n",
    "translated = GoogleTranslator(source='auto', target='en').translate(to_translate)\n",
    "translated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d33e8667d8178efa7deef3a321041744c51a72df3bf15f8600944ed1b5e39ebd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
